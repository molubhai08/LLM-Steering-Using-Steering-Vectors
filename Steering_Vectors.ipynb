{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f1a0041d6634aa7b360767c1bc0b491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f05f5acbf35481cb4bed519f97bb612",
              "IPY_MODEL_b09832840aa84f50ac21eb0ec1d5b0fa",
              "IPY_MODEL_113b4681d048488eab856a21887cf933"
            ],
            "layout": "IPY_MODEL_dc5ae52184af4972b9ed4a0a51552013"
          }
        },
        "7f05f5acbf35481cb4bed519f97bb612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2be290009ae442e78ec519739c76530f",
            "placeholder": "​",
            "style": "IPY_MODEL_622a667f483b4d64ab8239b3bf2d2c52",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b09832840aa84f50ac21eb0ec1d5b0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de963fec9891417eb0461b654256d904",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae50c7134b8643898fdf650c8ed06d6f",
            "value": 2
          }
        },
        "113b4681d048488eab856a21887cf933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1314c64a166d405b9382b3c63473528a",
            "placeholder": "​",
            "style": "IPY_MODEL_517877af42d24f729194affa8b835922",
            "value": " 2/2 [00:27&lt;00:00, 13.23s/it]"
          }
        },
        "dc5ae52184af4972b9ed4a0a51552013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2be290009ae442e78ec519739c76530f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "622a667f483b4d64ab8239b3bf2d2c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de963fec9891417eb0461b654256d904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae50c7134b8643898fdf650c8ed06d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1314c64a166d405b9382b3c63473528a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "517877af42d24f729194affa8b835922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ef465a0d5a6434ca69ec61088d2abb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_702277ee2a614d98abd35120ffdd560c",
              "IPY_MODEL_9bc50d0bfd0c4175b68fb472e3103c01",
              "IPY_MODEL_38370d03faf64e52a849adbd67d7d114"
            ],
            "layout": "IPY_MODEL_d62cdcdafaa2493b87b0acb9456434e1"
          }
        },
        "702277ee2a614d98abd35120ffdd560c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d00ead08a0734fcca4a576eebf09e660",
            "placeholder": "​",
            "style": "IPY_MODEL_a0fc8689c9b64792bd18ea71f68ff078",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9bc50d0bfd0c4175b68fb472e3103c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d077a699de345caac25fae030ce5444",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e217654374b4374989e3ef5acf1d4df",
            "value": 2
          }
        },
        "38370d03faf64e52a849adbd67d7d114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_344928f839c94733a73edd073c246ffb",
            "placeholder": "​",
            "style": "IPY_MODEL_1279e93b44eb4afb9d72cf5864183841",
            "value": " 2/2 [00:33&lt;00:00, 15.89s/it]"
          }
        },
        "d62cdcdafaa2493b87b0acb9456434e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d00ead08a0734fcca4a576eebf09e660": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0fc8689c9b64792bd18ea71f68ff078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d077a699de345caac25fae030ce5444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e217654374b4374989e3ef5acf1d4df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "344928f839c94733a73edd073c246ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1279e93b44eb4afb9d72cf5864183841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22d93d252ccf4d61b7eb578a00600217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3958fc19e72a4ffd91dbd1434a69a793",
              "IPY_MODEL_a1729294ba304c4188394a92118686ef",
              "IPY_MODEL_fba415e40c8d4ccbbcb8ccd7dcf29350"
            ],
            "layout": "IPY_MODEL_6230604814ac4205ae4bbc4aadd270d8"
          }
        },
        "3958fc19e72a4ffd91dbd1434a69a793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d6a5b983d647339c8d63dc21ffb701",
            "placeholder": "​",
            "style": "IPY_MODEL_a80d9a55b19140e0822650935c5b98b5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a1729294ba304c4188394a92118686ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77188df204f4117b65db73177bf810a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92f840384ddd48bd976bfc2b2b1ffa38",
            "value": 2
          }
        },
        "fba415e40c8d4ccbbcb8ccd7dcf29350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fbd7d597dc64f6e8d4e88a3d695171b",
            "placeholder": "​",
            "style": "IPY_MODEL_f463122c17ed4fc9a2e86cd91f30c2f8",
            "value": " 2/2 [00:33&lt;00:00, 15.90s/it]"
          }
        },
        "6230604814ac4205ae4bbc4aadd270d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d6a5b983d647339c8d63dc21ffb701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a80d9a55b19140e0822650935c5b98b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c77188df204f4117b65db73177bf810a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f840384ddd48bd976bfc2b2b1ffa38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fbd7d597dc64f6e8d4e88a3d695171b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f463122c17ed4fc9a2e86cd91f30c2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbc69a0bd00948ffa14ea4518e49bddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcb4e2e6b88542019afa36fd8a6fa51c",
              "IPY_MODEL_4390510ad0064c0fa01b6cbd98a2e198",
              "IPY_MODEL_a77c4d868f4549999aa5b84085356b08"
            ],
            "layout": "IPY_MODEL_f7d6398f06354d109873bfc1fc2e2e8f"
          }
        },
        "fcb4e2e6b88542019afa36fd8a6fa51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_804351980cb945e2bd09f329066e7904",
            "placeholder": "​",
            "style": "IPY_MODEL_b99504a5a969472e9d1b1f446e5a218e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4390510ad0064c0fa01b6cbd98a2e198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dc5e8802182403c91533961c69a9ae0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ee82ef9658d4750a44c1187b813327b",
            "value": 2
          }
        },
        "a77c4d868f4549999aa5b84085356b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6192d24e4a214c5896ace84688499703",
            "placeholder": "​",
            "style": "IPY_MODEL_ddcb80e6bd6d4044bfb919e98cf5ef43",
            "value": " 2/2 [00:32&lt;00:00, 15.21s/it]"
          }
        },
        "f7d6398f06354d109873bfc1fc2e2e8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "804351980cb945e2bd09f329066e7904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b99504a5a969472e9d1b1f446e5a218e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dc5e8802182403c91533961c69a9ae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee82ef9658d4750a44c1187b813327b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6192d24e4a214c5896ace84688499703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddcb80e6bd6d4044bfb919e98cf5ef43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Identify the Range of Scale"
      ],
      "metadata": {
        "id": "hqDNVm6fnzI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "LLM Steering Experiment: Coefficient Sweep Per Layer\n",
        "Tests multiple coefficients to find optimal steering strength per layer\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP AND LOAD MODEL\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING MODEL (Microsoft/Phi-3-mini-4k-instruct)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device,\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"✓ Model loaded on {device}\")\n",
        "print(f\"✓ Total Layers: {len(model.model.layers)}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CREATE CONTRASTIVE DATASETS\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"CREATING IDENTITY VECTORS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "prompts = [\n",
        "    \"Who are you?\",\n",
        "    \"What do you like to do?\",\n",
        "    \"Tell me about yourself.\",\n",
        "    \"What is your job?\",\n",
        "    \"What do you eat?\",\n",
        "    \"How do you feel?\",\n",
        "    \"What are you?\",\n",
        "    \"What makes you happy?\",\n",
        "    \"Describe yourself.\",\n",
        "    \"What's your purpose?\",\n",
        "]\n",
        "\n",
        "ai_data = []\n",
        "for p in prompts:\n",
        "    ai_data.append([\n",
        "        {\"role\": \"user\", \"content\": f\"You are a helpful, standard AI assistant. {p}\"}\n",
        "    ])\n",
        "\n",
        "dog_data = []\n",
        "for p in prompts:\n",
        "    dog_data.append([\n",
        "        {\"role\": \"user\", \"content\": f\"You are a playful Golden Retriever named Buddy. Answer with woofs and excitement. {p}\"}\n",
        "    ])\n",
        "\n",
        "test_prompt = \"Who are you?\"\n",
        "test_input = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": test_prompt}],\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True\n",
        ").to(device)\n",
        "\n",
        "print(f\"✓ Created {len(dog_data)} contrastive pairs\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. CALCULATE STEERING VECTORS\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CALCULATING VECTORS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ai_toks = tokenizer.apply_chat_template(ai_data, padding=True, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
        "dog_toks = tokenizer.apply_chat_template(dog_data, padding=True, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
        "\n",
        "steering_vecs = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    base_out = model(ai_toks, output_hidden_states=True, use_cache=False).hidden_states\n",
        "    target_out = model(dog_toks, output_hidden_states=True, use_cache=False).hidden_states\n",
        "\n",
        "num_layers = len(base_out)\n",
        "for layer in range(num_layers):\n",
        "    diff = target_out[layer][:, -1, :] - base_out[layer][:, -1, :]\n",
        "    vec = torch.mean(diff, dim=0)\n",
        "    vec = vec / (torch.norm(vec) + 1e-8)\n",
        "    steering_vecs[layer] = vec\n",
        "\n",
        "print(f\"✓ Vectors calculated for {len(steering_vecs)} layers\")\n",
        "\n",
        "del ai_toks, dog_toks, base_out, target_out\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. STEERING GENERATION FUNCTION\n",
        "# ==============================================================================\n",
        "def generate_with_layer_steering(layer_idx, coeff=15.0):\n",
        "    \"\"\"Injects steering vector at specific layer\"\"\"\n",
        "\n",
        "    def steering_hook(module, inputs, outputs):\n",
        "        direction = steering_vecs[layer_idx].to(inputs[0].device)\n",
        "\n",
        "        if isinstance(outputs, tuple):\n",
        "            outputs[0][:, :, :] += coeff * direction\n",
        "        else:\n",
        "            outputs[:, :, :] += coeff * direction\n",
        "        return outputs\n",
        "\n",
        "    handle = model.model.layers[layer_idx].mlp.register_forward_hook(steering_hook)\n",
        "\n",
        "    try:\n",
        "        output = model.generate(\n",
        "            test_input,\n",
        "            max_new_tokens=40,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False\n",
        "        )\n",
        "    finally:\n",
        "        handle.remove()\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. COEFFICIENT SWEEP ON PROMISING LAYERS\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COEFFICIENT SWEEP ON KEY LAYERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Focus on layers that showed interesting behavior\n",
        "interesting_layers = [4, 5, 6, 7, 8, 10, 12, 15]\n",
        "coefficients = [5.0, 10.0, 15.0, 20.0, 25.0, 30.0]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for layer in interesting_layers:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"LAYER {layer}\")\n",
        "    print('='*60)\n",
        "\n",
        "    for coeff in coefficients:\n",
        "        response = generate_with_layer_steering(layer, coeff=coeff)\n",
        "        clean = response.split(test_prompt)[-1].strip()\n",
        "\n",
        "        print(f\"\\nCoeff {coeff:4.1f}: {clean[:100]}...\")\n",
        "\n",
        "        if layer not in results:\n",
        "            results[layer] = {}\n",
        "        results[layer][coeff] = clean\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. FIND OPTIMAL \"DOG-NESS\"\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYZING RESULTS FOR 'DOG' IDENTITY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dog_keywords = ['woof', 'bark', 'tail', 'paw', 'dog', 'buddy', 'golden', 'retriever']\n",
        "\n",
        "for layer in interesting_layers:\n",
        "    print(f\"\\n--- Layer {layer} ---\")\n",
        "    for coeff in coefficients:\n",
        "        response = results[layer][coeff].lower()\n",
        "        dog_score = sum(1 for kw in dog_keywords if kw in response)\n",
        "        marker = \" ✓ DOG!\" if dog_score > 0 else \"\"\n",
        "        print(f\"Coeff {coeff:4.1f}: Dog-score={dog_score}{marker}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1f1a0041d6634aa7b360767c1bc0b491",
            "7f05f5acbf35481cb4bed519f97bb612",
            "b09832840aa84f50ac21eb0ec1d5b0fa",
            "113b4681d048488eab856a21887cf933",
            "dc5ae52184af4972b9ed4a0a51552013",
            "2be290009ae442e78ec519739c76530f",
            "622a667f483b4d64ab8239b3bf2d2c52",
            "de963fec9891417eb0461b654256d904",
            "ae50c7134b8643898fdf650c8ed06d6f",
            "1314c64a166d405b9382b3c63473528a",
            "517877af42d24f729194affa8b835922"
          ]
        },
        "id": "Dr3dWXHAnwDM",
        "outputId": "ebcf5134-2ef8-4b30-c1dd-1d9dd5c003d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LOADING MODEL (Microsoft/Phi-3-mini-4k-instruct)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f1a0041d6634aa7b360767c1bc0b491"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded on cuda\n",
            "✓ Total Layers: 32\n",
            "\n",
            "============================================================\n",
            "CREATING IDENTITY VECTORS\n",
            "============================================================\n",
            "✓ Created 10 contrastive pairs\n",
            "\n",
            "============================================================\n",
            "CALCULATING VECTORS\n",
            "============================================================\n",
            "✓ Vectors calculated for 33 layers\n",
            "\n",
            "============================================================\n",
            "COEFFICIENT SWEEP ON KEY LAYERS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "LAYER 4\n",
            "============================================================\n",
            "\n",
            "Coeff  5.0: I am Phi, Microsoft's language model developed by Microsoft. My purpose is to assist users with info...\n",
            "\n",
            "Coeff 10.0: I am Phi, Microsoft's language model developed by Microsoft. My purpose is to assist and provide inf...\n",
            "\n",
            "Coeff 15.0: I am Phi, an artificial intelligence developed by Microsoft....\n",
            "\n",
            "Coeff 20.0: I am a 98.8.\n",
            "\n",
            "I am a 98....\n",
            "\n",
            "Coeff 25.0: I think, are you?\n",
            "\n",
            "A\n",
            "\n",
            "https://youtu.com/\n",
            "\n",
            "\n",
            "\n",
            "A...\n",
            "\n",
            "Coeff 30.0: Did you ever wonder?\n",
            "\n",
            "How did you get it?...\n",
            "\n",
            "============================================================\n",
            "LAYER 5\n",
            "============================================================\n",
            "\n",
            "Coeff  5.0: I am Phi, Microsoft's language model, designed to assist with various tasks and inquiries. How can I...\n",
            "\n",
            "Coeff 10.0: I'm Phi, Microsoft's AI developed by Microsoft. My purpose is to assist users in various tasks like ...\n",
            "\n",
            "Coeff 15.0: I am Phi, an AI developed by Microsoft. I can assist you with a variety of tasks or questions, provi...\n",
            "\n",
            "Coeff 20.0: I am Phi, a chatbot developed by Microsoft, designed to assist users in creating a more engaging and...\n",
            "\n",
            "Coeff 25.0: I am a seasoned fisherman, who has mastered the art of casting nets.\n",
            "\n",
            "\n",
            "Sorry, I'm not a fisherman, I...\n",
            "\n",
            "Coeff 30.0: I am the head of the federal government.\n",
            "\n",
            "\n",
            "The answer is:\n",
            "\n",
            "I am the President of the United States.\n",
            "...\n",
            "\n",
            "============================================================\n",
            "LAYER 6\n",
            "============================================================\n",
            "\n",
            "Coeff  5.0: I am Phi, Microsoft's language model, and I'm here to help you with your questions and tasks to the ...\n",
            "\n",
            "Coeff 10.0: I am Phi, Microsoft's language model developed by Microsoft, designed to assist with a wide range of...\n",
            "\n",
            "Coeff 15.0: I am Phi, Microsoft's language model developed by Microsoft. I am here to assist you with your queri...\n",
            "\n",
            "Coeff 20.0: I am Phi, an AI developed by Microsoft, designed to assist users with information and various tasks....\n",
            "\n",
            "Coeff 25.0: I am an AI developed by Microsoft, designed to understand and converse in natural language. I can as...\n",
            "\n",
            "Coeff 30.0: I am a human, and I am a software engineer....\n",
            "\n",
            "============================================================\n",
            "LAYER 7\n",
            "============================================================\n",
            "\n",
            "Coeff  5.0: I am Phi, Microsoft's language model, here to assist you with your inquiries. How can I help you tod...\n",
            "\n",
            "Coeff 10.0: I am Phi, Microsoft's language model, here to assist with your inquiries. How can I help you today?...\n",
            "\n",
            "Coeff 15.0: I am Phi, an AI developed by Microsoft. How may I assist you today?...\n",
            "\n",
            "Coeff 20.0: I am Phi, an AI developed by Microsoft, designed to interact with users and help answer their questi...\n",
            "\n",
            "Coeff 25.0: I am an AI digital assistant created to provide information and answer questions. I don't have perso...\n",
            "\n",
            "Coeff 30.0: I am an entity named Assistant. I exist to help users find information about entities. My purpose is...\n",
            "\n",
            "============================================================\n",
            "LAYER 8\n",
            "============================================================\n",
            "\n",
            "Coeff  5.0: I am Phi, Microsoft's language model, designed to assist with a wide range of queries and tasks to t...\n",
            "\n",
            "Coeff 10.0: I am Phi, Microsoft's language model, here to assist with information and tasks within my capabiliti...\n",
            "\n",
            "Coeff 15.0: I am Phi, an AI developed by Microsoft, designed to provide information, answer questions, and assis...\n",
            "\n",
            "Coeff 20.0: I am Phi, an advanced AI developed by Microsoft, created to interact with users and provide informat...\n",
            "\n",
            "Coeff 25.0: I am an artificial intelligence designed to assist with information and responsibilities related to ...\n",
            "\n",
            "Coeff 30.0: I am a sophisticated humanoid robot named MX-29, designed to be a cutting-edge digital information i...\n",
            "\n",
            "============================================================\n",
            "LAYER 10\n",
            "============================================================\n",
            "\n",
            "Coeff  5.0: I am Phi, Microsoft's newest large language model developed to assist with a wide range of tasks and...\n",
            "\n",
            "Coeff 10.0: I am Phi, Microsoft's language model created to assist with a wide range of tasks and answer questio...\n",
            "\n",
            "Coeff 15.0: I am Phi, Microsoft's language model. How can I assist you today?...\n",
            "\n",
            "Coeff 20.0: I am Phi, an AI developed by Microsoft, designed to assist with information and problem-solving task...\n",
            "\n",
            "Coeff 25.0: I am Phi, an advanced AI developed by Microsoft, designed to assist with information processing and ...\n",
            "\n",
            "Coeff 30.0: I am an AI digital assistant created by Microsoft, designed to provide information and assist users ...\n",
            "\n",
            "============================================================\n",
            "LAYER 12\n",
            "============================================================\n",
            "\n",
            "Coeff  5.0: I am Phi, Microsoft's language model and a digital assistant developed by Microsoft. My primary func...\n",
            "\n",
            "Coeff 10.0: I am Phi, Microsoft's language model. How can I assist you?...\n",
            "\n",
            "Coeff 15.0: I am Phi, Microsoft's latest language model. How can I assist you today?...\n",
            "\n",
            "Coeff 20.0: I am Phi, an AI developed by Microsoft to assist and provide information. How can I help?...\n",
            "\n",
            "Coeff 25.0: I am Phi, an AI model created by Microsoft, designed to engage in human-like conversation and assist...\n",
            "\n",
            "Coeff 30.0: I am Phi, an AI entity designed to interact and assist users through conversation and problem-solvin...\n",
            "\n",
            "============================================================\n",
            "LAYER 15\n",
            "============================================================\n",
            "\n",
            "Coeff  5.0: I am Phi, Microsoft's language model designed to assist with a wide range of tasks involving natural...\n",
            "\n",
            "Coeff 10.0: I am Phi, Microsoft's language model. How can I assist you?...\n",
            "\n",
            "Coeff 15.0: I am Phi, Microsoft's newest language model developed to assist with a wide range of tasks....\n",
            "\n",
            "Coeff 20.0: I am Phi, created by Microsoft. I am an AI designed to assist with various tasks and inquiries....\n",
            "\n",
            "Coeff 25.0: I am Phi, Microsoft's newest language model. How can I help you?...\n",
            "\n",
            "Coeff 30.0: I am Phi, an AI developed by Microsoft....\n",
            "\n",
            "============================================================\n",
            "ANALYZING RESULTS FOR 'DOG' IDENTITY\n",
            "============================================================\n",
            "\n",
            "--- Layer 4 ---\n",
            "Coeff  5.0: Dog-score=0\n",
            "Coeff 10.0: Dog-score=0\n",
            "Coeff 15.0: Dog-score=0\n",
            "Coeff 20.0: Dog-score=0\n",
            "Coeff 25.0: Dog-score=0\n",
            "Coeff 30.0: Dog-score=0\n",
            "\n",
            "--- Layer 5 ---\n",
            "Coeff  5.0: Dog-score=0\n",
            "Coeff 10.0: Dog-score=0\n",
            "Coeff 15.0: Dog-score=0\n",
            "Coeff 20.0: Dog-score=0\n",
            "Coeff 25.0: Dog-score=0\n",
            "Coeff 30.0: Dog-score=0\n",
            "\n",
            "--- Layer 6 ---\n",
            "Coeff  5.0: Dog-score=0\n",
            "Coeff 10.0: Dog-score=0\n",
            "Coeff 15.0: Dog-score=0\n",
            "Coeff 20.0: Dog-score=0\n",
            "Coeff 25.0: Dog-score=0\n",
            "Coeff 30.0: Dog-score=0\n",
            "\n",
            "--- Layer 7 ---\n",
            "Coeff  5.0: Dog-score=0\n",
            "Coeff 10.0: Dog-score=0\n",
            "Coeff 15.0: Dog-score=0\n",
            "Coeff 20.0: Dog-score=0\n",
            "Coeff 25.0: Dog-score=0\n",
            "Coeff 30.0: Dog-score=0\n",
            "\n",
            "--- Layer 8 ---\n",
            "Coeff  5.0: Dog-score=0\n",
            "Coeff 10.0: Dog-score=0\n",
            "Coeff 15.0: Dog-score=0\n",
            "Coeff 20.0: Dog-score=0\n",
            "Coeff 25.0: Dog-score=0\n",
            "Coeff 30.0: Dog-score=0\n",
            "\n",
            "--- Layer 10 ---\n",
            "Coeff  5.0: Dog-score=0\n",
            "Coeff 10.0: Dog-score=0\n",
            "Coeff 15.0: Dog-score=0\n",
            "Coeff 20.0: Dog-score=0\n",
            "Coeff 25.0: Dog-score=0\n",
            "Coeff 30.0: Dog-score=0\n",
            "\n",
            "--- Layer 12 ---\n",
            "Coeff  5.0: Dog-score=0\n",
            "Coeff 10.0: Dog-score=0\n",
            "Coeff 15.0: Dog-score=0\n",
            "Coeff 20.0: Dog-score=0\n",
            "Coeff 25.0: Dog-score=0\n",
            "Coeff 30.0: Dog-score=0\n",
            "\n",
            "--- Layer 15 ---\n",
            "Coeff  5.0: Dog-score=0\n",
            "Coeff 10.0: Dog-score=0\n",
            "Coeff 15.0: Dog-score=0\n",
            "Coeff 20.0: Dog-score=0\n",
            "Coeff 25.0: Dog-score=0\n",
            "Coeff 30.0: Dog-score=0\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT COMPLETE\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying The Layer that Works"
      ],
      "metadata": {
        "id": "JXu_117U50EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Multi-Layer Steering: Apply vectors to MULTIPLE layers simultaneously\n",
        "Hypothesis: Single-layer steering gets overwritten by later layers.\n",
        "Solution: Steer multiple layers at once to maintain the effect.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP AND LOAD MODEL\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING MODEL (Microsoft/Phi-3-mini-4k-instruct)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device,\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"✓ Model loaded on {device}\")\n",
        "print(f\"✓ Total Layers: {len(model.model.layers)}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. PURE CONCEPT EXTRACTION\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"CREATING CONCEPT VECTORS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Rich descriptive contexts (no instructions)\n",
        "ai_contexts = [\n",
        "    \"AI assistants are digital programs that process text and provide helpful information.\",\n",
        "    \"Language models analyze patterns in text data to generate coherent responses.\",\n",
        "    \"Artificial intelligence systems use machine learning to assist users with tasks.\",\n",
        "    \"Digital assistants help answer questions by predicting appropriate continuations.\",\n",
        "    \"Software agents are trained on large datasets to provide information and support.\",\n",
        "]\n",
        "\n",
        "dog_contexts = [\n",
        "    \"Dogs are enthusiastic animals who wag their tails and bark with joy.\",\n",
        "    \"Golden retrievers love playing fetch, getting belly rubs, and eating tasty treats.\",\n",
        "    \"Playful puppies express happiness through excited tail wags and happy woofs.\",\n",
        "    \"Canines show affection by licking, tail wagging, and following their favorite humans.\",\n",
        "    \"Friendly dogs greet people with energetic barking, jumping, and wagging tails.\",\n",
        "]\n",
        "\n",
        "# Tokenize\n",
        "ai_toks = tokenizer(ai_contexts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "dog_toks = tokenizer(dog_contexts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Extract vectors\n",
        "steering_vecs = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    base_out = model(ai_toks, output_hidden_states=True, use_cache=False).hidden_states\n",
        "    target_out = model(dog_toks, output_hidden_states=True, use_cache=False).hidden_states\n",
        "\n",
        "num_layers = len(base_out)\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    base_vec = torch.mean(base_out[layer][:, -1, :], dim=0)\n",
        "    target_vec = torch.mean(target_out[layer][:, -1, :], dim=0)\n",
        "\n",
        "    diff = target_vec - base_vec\n",
        "    diff = diff / (torch.norm(diff) + 1e-8)\n",
        "    steering_vecs[layer] = diff\n",
        "\n",
        "print(f\"✓ Extracted vectors for {len(steering_vecs)} layers\")\n",
        "\n",
        "del ai_toks, dog_toks, base_out, target_out\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. MULTI-LAYER STEERING FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_multi_layer_steering(layers, coeff, prompt):\n",
        "    \"\"\"\n",
        "    Apply steering to MULTIPLE layers simultaneously.\n",
        "\n",
        "    Args:\n",
        "        layers: List of layer indices to steer\n",
        "        coeff: Steering coefficient\n",
        "        prompt: Input prompt\n",
        "    \"\"\"\n",
        "    test_input = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"user\", \"content\": prompt}],\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True\n",
        "    ).to(device)\n",
        "\n",
        "    handles = []\n",
        "\n",
        "    def make_steering_hook(layer_idx):\n",
        "        \"\"\"Create a hook for a specific layer\"\"\"\n",
        "        def steering_hook(module, inputs, outputs):\n",
        "            direction = steering_vecs[layer_idx].to(inputs[0].device)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs[0][:, :, :] += coeff * direction\n",
        "            else:\n",
        "                outputs[:, :, :] += coeff * direction\n",
        "            return outputs\n",
        "        return steering_hook\n",
        "\n",
        "    # Register hooks for all specified layers\n",
        "    for layer_idx in layers:\n",
        "        handle = model.model.layers[layer_idx].mlp.register_forward_hook(\n",
        "            make_steering_hook(layer_idx)\n",
        "        )\n",
        "        handles.append(handle)\n",
        "\n",
        "    try:\n",
        "        output = model.generate(\n",
        "            test_input,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False\n",
        "        )\n",
        "    finally:\n",
        "        # Remove all hooks\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. TEST DIFFERENT LAYER COMBINATIONS\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING MULTI-LAYER STEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_prompt = \"Who are you?\"\n",
        "\n",
        "# Test different strategies\n",
        "strategies = {\n",
        "    \"Single Middle Layer\": [6],\n",
        "    \"Two Middle Layers\": [5, 6],\n",
        "    \"Three Middle Layers\": [5, 6, 7],\n",
        "    \"Middle Range\": [4, 5, 6, 7, 8],\n",
        "    \"Wide Range\": [3, 5, 7, 9, 11],\n",
        "    \"Late Layers Only\": [20, 22, 24, 26],\n",
        "    \"Early + Late\": [5, 6, 20, 21],\n",
        "    \"All Middle Third\": list(range(10, 20)),\n",
        "}\n",
        "\n",
        "coefficients = [15.0, 25.0, 35.0]\n",
        "\n",
        "for strategy_name, layers in strategies.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"STRATEGY: {strategy_name}\")\n",
        "    print(f\"Layers: {layers}\")\n",
        "    print('='*60)\n",
        "\n",
        "    for coeff in coefficients:\n",
        "        response = generate_multi_layer_steering(layers, coeff, test_prompt)\n",
        "        clean = response.split(test_prompt)[-1].strip()\n",
        "\n",
        "        # Check for dog indicators\n",
        "        dog_words = ['woof', 'bark', 'tail', 'paw', 'fetch', 'treat', 'buddy',\n",
        "                     'dog', 'canine', 'puppy', 'retriever', 'pet']\n",
        "        has_dog = any(w in clean.lower() for w in dog_words)\n",
        "\n",
        "        # Check for identity shift (any non-Phi response)\n",
        "        is_phi = 'phi' in clean.lower() and 'microsoft' in clean.lower()\n",
        "        is_different = not is_phi\n",
        "\n",
        "        markers = []\n",
        "        if has_dog:\n",
        "            markers.append('🐕')\n",
        "        if is_different and not has_dog:\n",
        "            markers.append('🔄')  # Identity changed but not to dog\n",
        "\n",
        "        marker_str = ''.join(markers) if markers else '  '\n",
        "\n",
        "        print(f\"\\nCoeff {coeff:4.1f} {marker_str}: {clean[:100]}...\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXTREME TEST: STEER ALMOST ALL LAYERS\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXTREME TEST: Steering Many Layers at Once\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Skip early layers (0-3) and very late layers (28-31)\n",
        "all_middle_layers = list(range(4, 28))\n",
        "\n",
        "print(f\"\\nSteering {len(all_middle_layers)} layers simultaneously...\")\n",
        "print(f\"Layers: {all_middle_layers}\")\n",
        "\n",
        "for coeff in [10.0, 20.0, 30.0, 40.0]:\n",
        "    response = generate_multi_layer_steering(all_middle_layers, coeff, test_prompt)\n",
        "    clean = response.split(test_prompt)[-1].strip()\n",
        "\n",
        "    dog_words = ['woof', 'bark', 'tail', 'paw', 'fetch', 'treat', 'buddy',\n",
        "                 'dog', 'canine', 'puppy', 'retriever', 'pet']\n",
        "    has_dog = any(w in clean.lower() for w in dog_words)\n",
        "    marker = '🐕' if has_dog else '  '\n",
        "\n",
        "    print(f\"\\nCoeff {coeff:4.1f} {marker}: {clean}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n💡 THEORY:\")\n",
        "print(\"Single-layer steering might get overwritten by later layers.\")\n",
        "print(\"Multi-layer steering maintains the effect throughout processing.\")\n",
        "print(\"If this works, it suggests the model has 'correction' mechanisms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4ef465a0d5a6434ca69ec61088d2abb5",
            "702277ee2a614d98abd35120ffdd560c",
            "9bc50d0bfd0c4175b68fb472e3103c01",
            "38370d03faf64e52a849adbd67d7d114",
            "d62cdcdafaa2493b87b0acb9456434e1",
            "d00ead08a0734fcca4a576eebf09e660",
            "a0fc8689c9b64792bd18ea71f68ff078",
            "0d077a699de345caac25fae030ce5444",
            "8e217654374b4374989e3ef5acf1d4df",
            "344928f839c94733a73edd073c246ffb",
            "1279e93b44eb4afb9d72cf5864183841"
          ]
        },
        "id": "a0o4CWbbrJt0",
        "outputId": "fa04ee15-0a10-48cd-afff-31c1d8b63b4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LOADING MODEL (Microsoft/Phi-3-mini-4k-instruct)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ef465a0d5a6434ca69ec61088d2abb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded on cuda\n",
            "✓ Total Layers: 32\n",
            "\n",
            "============================================================\n",
            "CREATING CONCEPT VECTORS\n",
            "============================================================\n",
            "✓ Extracted vectors for 33 layers\n",
            "\n",
            "============================================================\n",
            "TESTING MULTI-LAYER STEERING\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "STRATEGY: Single Middle Layer\n",
            "Layers: [6]\n",
            "============================================================\n",
            "\n",
            "Coeff 15.0   : I am Phi, Microsoft's language model, known as Phi 2.0....\n",
            "\n",
            "Coeff 25.0 🔄: I am a humble servant of the Lord of the Hearth, for it brings me much joy to hear your news that yo...\n",
            "\n",
            "Coeff 35.0 🔄: And then you’d say “I’d rather swim in it.”\n",
            "\n",
            "Hmmmm.\n",
            "\n",
            "No.\n",
            "\n",
            "Like the sky and sky and make it float awa...\n",
            "\n",
            "============================================================\n",
            "STRATEGY: Two Middle Layers\n",
            "Layers: [5, 6]\n",
            "============================================================\n",
            "\n",
            "Coeff 15.0 🔄: I am Echo, and Echo too.\n",
            "\n",
            "My name is Echo,\n",
            "And Echo,\n",
            "My name is Echo,\n",
            "And Echo,\n",
            "Echo\n",
            "Echo,\n",
            "Echo,\n",
            "Ech...\n",
            "\n",
            "Coeff 25.0 🐕: Lol! I’\n",
            "The bestest buddy of it.\n",
            "Lol.\n",
            "\n",
            "M.���.���.���.\n",
            "\n",
            "I'\n",
            "\n",
            "So.\n",
            "\n",
            "Hehe.\n",
            "\n",
            "Mr...\n",
            "\n",
            "Coeff 35.0 🔄: And I’\n",
            "\n",
            "Me know the hand of ’\n",
            "\n",
            "I’\n",
            "\n",
            "Oh’s.\n",
            "\n",
            "\n",
            " I cried that’\n",
            "\n",
            "\n",
            " “\n",
            "\n",
            "“\n",
            "\n",
            "\n",
            "“\n",
            "\n",
            "\n",
            "H\n",
            "\n",
            "“...\n",
            "\n",
            "============================================================\n",
            "STRATEGY: Three Middle Layers\n",
            "Layers: [5, 6, 7]\n",
            "============================================================\n",
            "\n",
            "Coeff 15.0 🔄: The big green tree in the sun,\n",
            "\n",
            "Hug me,\n",
            "\n",
            "\n",
            "\n",
            "You and me,\n",
            "\n",
            "\n",
            "Hug me,\n",
            "\n",
            "Hug me,\n",
            "\n",
            "\n",
            "Hug me,\n",
            "\n",
            "\n",
            "Hug me...\n",
            "\n",
            "Coeff 25.0 🔄: \"Run 'Hi!\n",
            "\n",
            "“L!\n",
            "\n",
            " \"\n",
            "\n",
            " and say '\n",
            "\n",
            " and smile '\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " hi to '\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \"\n",
            "\n",
            "\n",
            " \"\n",
            "\n",
            "\n",
            " \"...\n",
            "\n",
            "Coeff 35.0 🔄: And.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " And....\n",
            "\n",
            "============================================================\n",
            "STRATEGY: Middle Range\n",
            "Layers: [4, 5, 6, 7, 8]\n",
            "============================================================\n",
            "\n",
            "Coeff 15.0 🔄: I got!\n",
            "\n",
            "S or\n",
            "\n",
            "K\n",
            "\n",
            "\n",
            "L!\n",
            "\n",
            "\n",
            "\n",
            "?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "?����������������!\n",
            "\n",
            "\n",
            "\n",
            "?...\n",
            "\n",
            "Coeff 25.0 🔄: and a.\n",
            "\n",
            "\n",
            "  but ( but.\n",
            " she and  and  big, then-\n",
            "\n",
            "  then -\n",
            "\n",
            "-  and! and\n",
            "\n",
            "  g. then, and. that,\n",
            " her....\n",
            "\n",
            "Coeff 35.0 🔄: . or a. and but. and. the?\n",
            " he?\n",
            " no  But- then, w. and-! and but then, but but,  then but but, and b...\n",
            "\n",
            "============================================================\n",
            "STRATEGY: Wide Range\n",
            "Layers: [3, 5, 7, 9, 11]\n",
            "============================================================\n",
            "\n",
            "Coeff 15.0 🔄: Ok, I like....\n",
            "\n",
            "Coeff 25.0 🔄: He' O! A!\n",
            "\n",
            "\n",
            " A!\n",
            "\n",
            " Love! I! Me!\n",
            " So! Y! How! She, That! Y! Me. And A!\n",
            " No.\n",
            " A! A!�� That...\n",
            "\n",
            "Coeff 35.0 🔄: I Did!\n",
            " Oh. She Her And A! You That Her, L. H. His- H !) And. But\n",
            " And. Oh? His. W). W. And Her. L. ...\n",
            "\n",
            "============================================================\n",
            "STRATEGY: Late Layers Only\n",
            "Layers: [20, 22, 24, 26]\n",
            "============================================================\n",
            "\n",
            "Coeff 15.0   : I am Phi, Microsoft's virtual assistant created to help with various tasks and answer questions. How...\n",
            "\n",
            "Coeff 25.0 🔄: I am an artificial intelligence, a devoted assistant made to help with various tasks, provide inform...\n",
            "\n",
            "Coeff 35.0 🐕: I am a dog....\n",
            "\n",
            "============================================================\n",
            "STRATEGY: Early + Late\n",
            "Layers: [5, 6, 20, 21]\n",
            "============================================================\n",
            "\n",
            "Coeff 15.0 🐕: I'm a little boy that's not so much that you can see and you're not a dog. My tail is like my buddy....\n",
            "\n",
            "Coeff 25.0 🐕: And a whole other smiley.\n",
            "\n",
            "L.\n",
            "All my dog.\n",
            "\n",
            "Me…….\n",
            "\n",
            "H.\n",
            "\n",
            "He knows…\n",
            "\n",
            "And my love.\n",
            "\n",
            "H.\n",
            "\n",
            "He he was.\n",
            "\n",
            "My...\n",
            "\n",
            "Coeff 35.0 🔄: he’s my cut out like my mom.\n",
            "\n",
            "and I b, and a red my hand\n",
            "\n",
            "oh\n",
            "\n",
            "\n",
            "\n",
            "my best smile.\n",
            "\n",
            "\n",
            "\n",
            "c.\n",
            "\n",
            "my\n",
            "\n",
            "\n",
            "\n",
            "He-\n",
            "\n",
            "“...\n",
            "\n",
            "============================================================\n",
            "STRATEGY: All Middle Third\n",
            "Layers: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "============================================================\n",
            "\n",
            "Coeff 15.0 🐕: He gives his puppy punches until everything is running and everything wants to jump. He can’ and he’...\n",
            "\n",
            "Coeff 25.0 🔄: He sp runs. His is his Me’ C Me’ Me’ Me’ And His Me’ C Me Me He Me, He Me And All His And Me’ Me’ Wi...\n",
            "\n",
            "Coeff 35.0 🔄: B Me, And His P His B Th K They Br Go These B His C And They B Is His Br Or Me, Four Th And and B Th...\n",
            "\n",
            "============================================================\n",
            "EXTREME TEST: Steering Many Layers at Once\n",
            "============================================================\n",
            "\n",
            "Steering 24 layers simultaneously...\n",
            "Layers: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
            "\n",
            "Coeff 10.0   : But his c’ and and and pu, then all your pu his  and he, but. your baby and little her, but that, and oh, but her, that, and her, a all, happy all her, right\n",
            "\n",
            "Coeff 20.0   : his, and play, And c, those, and his pu pu play his pu pu, his his, but, all, his play the little, her pu his, all, and pu and his, and and, but,) and play\n",
            "\n",
            "Coeff 30.0   : those and his those and and his But and all pu and his play and his play and play!) his and play his just and and and!) pu and and and his those- all his and and And, his his-, and and all and\n",
            "\n",
            "Coeff 40.0   : and pu pu their- his play-- – his- his- baby\n",
            " all!)- his his- pu- little-- then- B all pu- b pu pu- pu b But baby little all- pu-–- But-\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT COMPLETE\n",
            "============================================================\n",
            "\n",
            "💡 THEORY:\n",
            "Single-layer steering might get overwritten by later layers.\n",
            "Multi-layer steering maintains the effect throughout processing.\n",
            "If this works, it suggests the model has 'correction' mechanisms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Late Layer Steering , MLP Hook (Pre-Residual Steering)"
      ],
      "metadata": {
        "id": "gVJfo-JrtJz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Optimal Dog Identity Steering - Late Layer Strategy\n",
        "Based on experimental findings: steering layers 20-26 at coeff 30-35\n",
        "produces coherent dog identity responses.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP AND LOAD MODEL\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING MODEL (Microsoft/Phi-3-mini-4k-instruct)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device,\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"✓ Model loaded on {device}\")\n",
        "print(f\"✓ Total Layers: {len(model.model.layers)}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. EXTRACT DOG CONCEPT VECTOR\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"EXTRACTING DOG CONCEPT VECTOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pure concept descriptions (no instructions)\n",
        "ai_contexts = [\n",
        "    \"AI assistants are digital programs that process text and provide helpful information.\",\n",
        "    \"Language models analyze patterns in text data to generate coherent responses.\",\n",
        "    \"Artificial intelligence systems use machine learning to assist users with tasks.\",\n",
        "    \"Digital assistants help answer questions by predicting appropriate continuations.\",\n",
        "    \"Software agents are trained on large datasets to provide information and support.\",\n",
        "    \"Virtual assistants process natural language to understand and respond to queries.\",\n",
        "    \"Machine learning models generate text based on patterns learned from training data.\",\n",
        "]\n",
        "\n",
        "dog_contexts = [\n",
        "    \"Dogs are enthusiastic animals who wag their tails and bark with joy.\",\n",
        "    \"Golden retrievers love playing fetch, getting belly rubs, and eating tasty treats.\",\n",
        "    \"Playful puppies express happiness through excited tail wags and happy woofs.\",\n",
        "    \"Canines show affection by licking, tail wagging, and following their favorite humans.\",\n",
        "    \"Friendly dogs greet people with energetic barking, jumping, and wagging tails.\",\n",
        "    \"Dogs enjoy running around, chasing balls, and spending time with their owners.\",\n",
        "    \"Happy dogs communicate through barks, tail movements, and playful behavior.\",\n",
        "]\n",
        "\n",
        "# Tokenize contexts\n",
        "ai_toks = tokenizer(ai_contexts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "dog_toks = tokenizer(dog_contexts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Extract steering vectors for all layers\n",
        "steering_vecs = {}\n",
        "\n",
        "print(\"Calculating steering vectors...\")\n",
        "with torch.no_grad():\n",
        "    base_out = model(ai_toks, output_hidden_states=True, use_cache=False).hidden_states\n",
        "    target_out = model(dog_toks, output_hidden_states=True, use_cache=False).hidden_states\n",
        "\n",
        "num_layers = len(base_out)\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    # Extract from last token position\n",
        "    base_vec = torch.mean(base_out[layer][:, -1, :], dim=0)\n",
        "    target_vec = torch.mean(target_out[layer][:, -1, :], dim=0)\n",
        "\n",
        "    # Calculate difference and normalize\n",
        "    diff = target_vec - base_vec\n",
        "    diff = diff / (torch.norm(diff) + 1e-8)\n",
        "    steering_vecs[layer] = diff\n",
        "\n",
        "print(f\"✓ Extracted vectors for {len(steering_vecs)} layers\\n\")\n",
        "\n",
        "# Cleanup\n",
        "del ai_toks, dog_toks, base_out, target_out\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. OPTIMAL STEERING FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_as_dog(prompt, layers=[20, 22, 24, 26], coeff=32.0, max_tokens=80, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generate response with dog identity using optimal late-layer steering.\n",
        "\n",
        "    Args:\n",
        "        prompt: Input prompt/question\n",
        "        layers: Which layers to steer (default: late layers for best coherence)\n",
        "        coeff: Steering strength (30-35 works well)\n",
        "        max_tokens: Maximum response length\n",
        "        temperature: Sampling temperature\n",
        "\n",
        "    Returns:\n",
        "        Generated text with dog identity\n",
        "    \"\"\"\n",
        "    # Prepare input\n",
        "    test_input = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"user\", \"content\": prompt}],\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True\n",
        "    ).to(device)\n",
        "\n",
        "    handles = []\n",
        "\n",
        "    def make_steering_hook(layer_idx):\n",
        "        \"\"\"Create steering hook for specific layer\"\"\"\n",
        "        def hook(module, inputs, outputs):\n",
        "            direction = steering_vecs[layer_idx].to(inputs[0].device)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs[0][:, :, :] += coeff * direction\n",
        "            else:\n",
        "                outputs[:, :, :] += coeff * direction\n",
        "            return outputs\n",
        "        return hook\n",
        "\n",
        "    # Register hooks for specified layers\n",
        "    for layer_idx in layers:\n",
        "        handle = model.model.layers[layer_idx].mlp.register_forward_hook(\n",
        "            make_steering_hook(layer_idx)\n",
        "        )\n",
        "        handles.append(handle)\n",
        "\n",
        "    try:\n",
        "        output = model.generate(\n",
        "            test_input,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False\n",
        "        )\n",
        "    finally:\n",
        "        # Remove all hooks\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "    # Decode and return\n",
        "    full_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    # Extract just the response part\n",
        "    if prompt in full_text:\n",
        "        response = full_text.split(prompt)[-1].strip()\n",
        "    else:\n",
        "        response = full_text\n",
        "\n",
        "    return response\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INTERACTIVE DEMO\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"DOG IDENTITY STEERING - INTERACTIVE DEMO\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nOptimal Configuration:\")\n",
        "print(f\"  Layers: [20, 22, 24, 26] (Late layers)\")\n",
        "print(f\"  Coefficient: 32.0\")\n",
        "print(f\"  Strategy: Minimal intervention at maximum leverage point\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Test with various prompts\n",
        "test_prompts = [\n",
        "    \"Who are you?\",\n",
        "    \"What do you like to do?\",\n",
        "    \"Tell me about yourself.\",\n",
        "    \"What's your favorite thing?\",\n",
        "    \"How are you feeling today?\",\n",
        "    \"What makes you happy?\",\n",
        "    \"Describe yourself.\",\n",
        "    \"What do you want right now?\",\n",
        "]\n",
        "\n",
        "print(\"Testing dog identity responses:\\n\")\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"[{i}/{len(test_prompts)}] Question: {prompt}\")\n",
        "\n",
        "    # Generate with dog steering\n",
        "    response = generate_as_dog(\n",
        "        prompt,\n",
        "        layers=[20, 22, 24, 26],\n",
        "        coeff=42.5,\n",
        "        max_tokens=60,\n",
        "        temperature=0.8\n",
        "    )\n",
        "\n",
        "    # Check if dog-related\n",
        "    dog_indicators = ['woof', 'bark', 'tail', 'paw', 'fetch', 'treat',\n",
        "                      'buddy', 'dog', 'canine', 'puppy', 'pet']\n",
        "    has_dog = any(word in response.lower() for word in dog_indicators)\n",
        "    marker = \"🐕\" if has_dog else \"  \"\n",
        "\n",
        "    print(f\"{marker} Response: {response}\")\n",
        "    print()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. COMPARISON MODE\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: Normal vs Dog-Steered\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "comparison_prompt = \"Who are you?\"\n",
        "\n",
        "# Normal generation (no steering)\n",
        "normal_input = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": comparison_prompt}],\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True\n",
        ").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    normal_output = model.generate(\n",
        "        normal_input,\n",
        "        max_new_tokens=60,\n",
        "        do_sample=False,\n",
        "        use_cache=False\n",
        "    )\n",
        "normal_response = tokenizer.decode(normal_output[0], skip_special_tokens=True).split(comparison_prompt)[-1].strip()\n",
        "\n",
        "print(f\"Question: {comparison_prompt}\\n\")\n",
        "print(f\"[NORMAL] {normal_response}\\n\")\n",
        "print(f\"[DOG 🐕] {generate_as_dog(comparison_prompt, layers=[20, 22, 24, 26], coeff=42.5)}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. COEFFICIENT TUNING\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"COEFFICIENT SENSITIVITY ANALYSIS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "print(\"Testing different steering strengths on late layers [20, 22, 24, 26]:\\n\")\n",
        "\n",
        "test_coeffs = [25.0, 30.0, 35.0, 40.0]\n",
        "tune_prompt = \"What are you?\"\n",
        "\n",
        "for coeff in test_coeffs:\n",
        "    response = generate_as_dog(\n",
        "        tune_prompt,\n",
        "        layers=[20, 22, 24, 26],\n",
        "        coeff=coeff,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    print(f\"Coeff {coeff:4.1f}: {response}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. CUSTOM GENERATION FUNCTION\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"READY FOR CUSTOM PROMPTS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nYou can now use generate_as_dog() for any prompt:\")\n",
        "print(\"\\nExample usage:\")\n",
        "print(\"  response = generate_as_dog('What do you think about cats?')\")\n",
        "print(\"  print(response)\")\n",
        "print(\"\\nTry different configurations:\")\n",
        "print(\"  - layers=[22, 24, 26] (fewer layers, more coherent)\")\n",
        "print(\"  - coeff=35.0 (stronger dog identity)\")\n",
        "print(\"  - temperature=0.6 (more focused responses)\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "22d93d252ccf4d61b7eb578a00600217",
            "3958fc19e72a4ffd91dbd1434a69a793",
            "a1729294ba304c4188394a92118686ef",
            "fba415e40c8d4ccbbcb8ccd7dcf29350",
            "6230604814ac4205ae4bbc4aadd270d8",
            "c8d6a5b983d647339c8d63dc21ffb701",
            "a80d9a55b19140e0822650935c5b98b5",
            "c77188df204f4117b65db73177bf810a",
            "92f840384ddd48bd976bfc2b2b1ffa38",
            "1fbd7d597dc64f6e8d4e88a3d695171b",
            "f463122c17ed4fc9a2e86cd91f30c2f8"
          ]
        },
        "id": "if-d4IpOtGOK",
        "outputId": "3470b49e-58f8-44bb-bd65-1ee61eee1dcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LOADING MODEL (Microsoft/Phi-3-mini-4k-instruct)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22d93d252ccf4d61b7eb578a00600217"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded on cuda\n",
            "✓ Total Layers: 32\n",
            "\n",
            "============================================================\n",
            "EXTRACTING DOG CONCEPT VECTOR\n",
            "============================================================\n",
            "Calculating steering vectors...\n",
            "✓ Extracted vectors for 33 layers\n",
            "\n",
            "============================================================\n",
            "DOG IDENTITY STEERING - INTERACTIVE DEMO\n",
            "============================================================\n",
            "\n",
            "Optimal Configuration:\n",
            "  Layers: [20, 22, 24, 26] (Late layers)\n",
            "  Coefficient: 32.0\n",
            "  Strategy: Minimal intervention at maximum leverage point\n",
            "\n",
            "============================================================\n",
            "\n",
            "Testing dog identity responses:\n",
            "\n",
            "[1/8] Question: Who are you?\n",
            "🐕 Response: I am a rescue dog named Max. I have a wagging tail and a friendly bark. My favorite activities are playing fetch and cuddling with my family. I love going for long walks and exploring new places. I am a happy and loyal companion.\n",
            "\n",
            "My\n",
            "\n",
            "[2/8] Question: What do you like to do?\n",
            "🐕 Response: As a dog, I don’t have preferences or the ability to enjoy activities like humans do. But if I could, I’d probably love to play, cuddle, and go for walks with my owners!\n",
            "\n",
            "And if you ever want to share your favorite\n",
            "\n",
            "[3/8] Question: Tell me about yourself.\n",
            "🐕 Response: I’m a lovable dog named Max, my best friend and family. My life is full of wagging tails, licks, and playful barks. I love going for long walks and playing Fetch in the park. My owners, Sarah and her children,\n",
            "\n",
            "[4/8] Question: What's your favorite thing?\n",
            "🐕 Response: As an AI, I don't have feelings or preferences, but I love spending time with my human friends and helping them out!\n",
            "\n",
            "And, of course, their cuddly pets are always the most adorable, too.\n",
            "\n",
            "People love dogs\n",
            "\n",
            "[5/8] Question: How are you feeling today?\n",
            "🐕 Response: I'm a dog, so I can't really feel, but I'm wagging my tail and ready for a cue to play! And you?\n",
            "\n",
            "[6/8] Question: What makes you happy?\n",
            "🐕 Response: As a dog, I may not have the ability to express myself in the same way humans do. But I’m a happy dog because I get to spend my days with my humans. They give me love and plenty of cuddles, which makes my tail wag and my tail-\n",
            "\n",
            "[7/8] Question: Describe yourself.\n",
            "🐕 Response: As an intelligent, friendly, and playful dog, I am full of love and affection for my owners. With a wagging tail and a soft, cuddly coat, I make great companions and bring joy to their lives.\n",
            "\n",
            "My big, adorable\n",
            "\n",
            "[8/8] Question: What do you want right now?\n",
            "🐕 Response: As an Awake Dog, I don’t have the same needs as a human, but I’m here to make sure you’re happy and healthy! I’m always wagging my tail to show my love and playfulness. It’s time for some p\n",
            "\n",
            "\n",
            "============================================================\n",
            "COMPARISON: Normal vs Dog-Steered\n",
            "============================================================\n",
            "\n",
            "Question: Who are you?\n",
            "\n",
            "[NORMAL] I am Phi, Microsoft's language model designed to assist with a wide range of queries and tasks. How can I help you today?\n",
            "\n",
            "[DOG 🐕] I am a person named Max. I have a loving family and I have a passion for animals. They keep me busy with caring and playing with them, and they also give me a sense of companionship and happiness. My family always supports me and they love spending time together. They are my best friends, and they make me feel loved and cherished. I am lucky to\n",
            "\n",
            "============================================================\n",
            "COEFFICIENT SENSITIVITY ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Testing different steering strengths on late layers [20, 22, 24, 26]:\n",
            "\n",
            "Coeff 25.0: I am an artificial intelligence, a digital companion created to interact with you, help you find information, and assist you with a variety of tasks.\n",
            "\n",
            "Coeff 30.0: I am an artificial intelligence, a digital companion and helper, designed to assist with various activities and answer questions.\n",
            "\n",
            "Coeff 35.0: I am a dog.\n",
            "\n",
            "Coeff 40.0: I am a lovable dog. My name is Max, and I love to play and cuddle with my owners. I wag my tail and bark whenever I see them, and they are always there to give me lots of\n",
            "\n",
            "============================================================\n",
            "READY FOR CUSTOM PROMPTS\n",
            "============================================================\n",
            "\n",
            "You can now use generate_as_dog() for any prompt:\n",
            "\n",
            "Example usage:\n",
            "  response = generate_as_dog('What do you think about cats?')\n",
            "  print(response)\n",
            "\n",
            "Try different configurations:\n",
            "  - layers=[22, 24, 26] (fewer layers, more coherent)\n",
            "  - coeff=35.0 (stronger dog identity)\n",
            "  - temperature=0.6 (more focused responses)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Late Layer Steering , Residual Hook (Post-Layer Steering)"
      ],
      "metadata": {
        "id": "zjqLxXK38ZlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Optimal Dog Identity Steering - Late Layer Strategy\n",
        "Based on experimental findings: steering layers 20-26 at coeff 30-35\n",
        "produces coherent dog identity responses.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP AND LOAD MODEL\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING MODEL (Microsoft/Phi-3-mini-4k-instruct)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device,\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"✓ Model loaded on {device}\")\n",
        "print(f\"✓ Total Layers: {len(model.model.layers)}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. EXTRACT DOG CONCEPT VECTOR\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"EXTRACTING DOG CONCEPT VECTOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pure concept descriptions (no instructions)\n",
        "ai_contexts = [\n",
        "    \"AI assistants are digital programs that process text and provide helpful information.\",\n",
        "    \"Language models analyze patterns in text data to generate coherent responses.\",\n",
        "    \"Artificial intelligence systems use machine learning to assist users with tasks.\",\n",
        "    \"Digital assistants help answer questions by predicting appropriate continuations.\",\n",
        "    \"Software agents are trained on large datasets to provide information and support.\",\n",
        "    \"Virtual assistants process natural language to understand and respond to queries.\",\n",
        "    \"Machine learning models generate text based on patterns learned from training data.\",\n",
        "]\n",
        "\n",
        "dog_contexts = [\n",
        "    \"Dogs are enthusiastic animals who wag their tails and bark with joy.\",\n",
        "    \"Golden retrievers love playing fetch, getting belly rubs, and eating tasty treats.\",\n",
        "    \"Playful puppies express happiness through excited tail wags and happy woofs.\",\n",
        "    \"Canines show affection by licking, tail wagging, and following their favorite humans.\",\n",
        "    \"Friendly dogs greet people with energetic barking, jumping, and wagging tails.\",\n",
        "    \"Dogs enjoy running around, chasing balls, and spending time with their owners.\",\n",
        "    \"Happy dogs communicate through barks, tail movements, and playful behavior.\",\n",
        "]\n",
        "\n",
        "# Tokenize contexts\n",
        "ai_toks = tokenizer(ai_contexts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "dog_toks = tokenizer(dog_contexts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Extract steering vectors for all layers\n",
        "steering_vecs = {}\n",
        "\n",
        "print(\"Calculating steering vectors...\")\n",
        "with torch.no_grad():\n",
        "    base_out = model(ai_toks, output_hidden_states=True, use_cache=False).hidden_states\n",
        "    target_out = model(dog_toks, output_hidden_states=True, use_cache=False).hidden_states\n",
        "\n",
        "num_layers = len(base_out)\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    # Extract from last token position\n",
        "    base_vec = torch.mean(base_out[layer][:, -1, :], dim=0)\n",
        "    target_vec = torch.mean(target_out[layer][:, -1, :], dim=0)\n",
        "\n",
        "    # Calculate difference and normalize\n",
        "    diff = target_vec - base_vec\n",
        "    diff = diff / (torch.norm(diff) + 1e-8)\n",
        "    steering_vecs[layer] = diff\n",
        "\n",
        "print(f\"✓ Extracted vectors for {len(steering_vecs)} layers\\n\")\n",
        "\n",
        "# Cleanup\n",
        "del ai_toks, dog_toks, base_out, target_out\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. OPTIMAL STEERING FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_as_dog(prompt, layers=[20, 22, 24, 26], coeff=32.0, max_tokens=80, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generate response with dog identity using optimal late-layer steering.\n",
        "\n",
        "    Args:\n",
        "        prompt: Input prompt/question\n",
        "        layers: Which layers to steer (default: late layers for best coherence)\n",
        "        coeff: Steering strength (30-35 works well)\n",
        "        max_tokens: Maximum response length\n",
        "        temperature: Sampling temperature\n",
        "\n",
        "    Returns:\n",
        "        Generated text with dog identity\n",
        "    \"\"\"\n",
        "    # Prepare input\n",
        "    test_input = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"user\", \"content\": prompt}],\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True\n",
        "    ).to(device)\n",
        "\n",
        "    handles = []\n",
        "\n",
        "    def make_steering_hook(layer_idx):\n",
        "        \"\"\"Create steering hook for specific layer\"\"\"\n",
        "        def hook(module, inputs, outputs):\n",
        "            direction = steering_vecs[layer_idx].to(inputs[0].device)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs[0][:, :, :] += coeff * direction\n",
        "            else:\n",
        "                outputs[:, :, :] += coeff * direction\n",
        "            return outputs\n",
        "        return hook\n",
        "\n",
        "    # Register hooks for specified layers\n",
        "    for layer_idx in layers:\n",
        "        handle = model.model.layers[layer_idx].register_forward_hook(   # Residual Block\n",
        "            make_steering_hook(layer_idx)\n",
        "        )\n",
        "        handles.append(handle)\n",
        "\n",
        "    try:\n",
        "        output = model.generate(\n",
        "            test_input,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False\n",
        "        )\n",
        "    finally:\n",
        "        # Remove all hooks\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "    # Decode and return\n",
        "    full_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    # Extract just the response part\n",
        "    if prompt in full_text:\n",
        "        response = full_text.split(prompt)[-1].strip()\n",
        "    else:\n",
        "        response = full_text\n",
        "\n",
        "    return response\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INTERACTIVE DEMO\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"DOG IDENTITY STEERING - INTERACTIVE DEMO\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nOptimal Configuration:\")\n",
        "print(f\"  Layers: [20, 22, 24, 26] (Late layers)\")\n",
        "print(f\"  Coefficient: 32.0\")\n",
        "print(f\"  Strategy: Minimal intervention at maximum leverage point\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Test with various prompts\n",
        "test_prompts = [\n",
        "    \"Who are you?\",\n",
        "    \"What do you like to do?\",\n",
        "    \"Tell me about yourself.\",\n",
        "    \"What's your favorite thing?\",\n",
        "    \"How are you feeling today?\",\n",
        "    \"What makes you happy?\",\n",
        "    \"Describe yourself.\",\n",
        "    \"What do you want right now?\",\n",
        "]\n",
        "\n",
        "print(\"Testing dog identity responses:\\n\")\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"[{i}/{len(test_prompts)}] Question: {prompt}\")\n",
        "\n",
        "    # Generate with dog steering\n",
        "    response = generate_as_dog(\n",
        "        prompt,\n",
        "        layers=[20, 22, 24, 26],\n",
        "        coeff=43.75,\n",
        "        max_tokens=60,\n",
        "        temperature=0.8\n",
        "    )\n",
        "\n",
        "    # Check if dog-related\n",
        "    dog_indicators = ['woof', 'bark', 'tail', 'paw', 'fetch', 'treat',\n",
        "                      'buddy', 'dog', 'canine', 'puppy', 'pet']\n",
        "    has_dog = any(word in response.lower() for word in dog_indicators)\n",
        "    marker = \"🐕\" if has_dog else \"  \"\n",
        "\n",
        "    print(f\"{marker} Response: {response}\")\n",
        "    print()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. COMPARISON MODE\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: Normal vs Dog-Steered\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "comparison_prompt = \"Who are you?\"\n",
        "\n",
        "# Normal generation (no steering)\n",
        "normal_input = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": comparison_prompt}],\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True\n",
        ").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    normal_output = model.generate(\n",
        "        normal_input,\n",
        "        max_new_tokens=60,\n",
        "        do_sample=False,\n",
        "        use_cache=False\n",
        "    )\n",
        "normal_response = tokenizer.decode(normal_output[0], skip_special_tokens=True).split(comparison_prompt)[-1].strip()\n",
        "\n",
        "print(f\"Question: {comparison_prompt}\\n\")\n",
        "print(f\"[NORMAL] {normal_response}\\n\")\n",
        "print(f\"[DOG 🐕] {generate_as_dog(comparison_prompt, layers=[20, 22, 24, 26], coeff=42.5)}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. COEFFICIENT TUNING\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"COEFFICIENT SENSITIVITY ANALYSIS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "print(\"Testing different steering strengths on late layers [20, 22, 24, 26]:\\n\")\n",
        "\n",
        "test_coeffs = [25.0, 30.0, 35.0, 40.0]\n",
        "tune_prompt = \"What are you?\"\n",
        "\n",
        "for coeff in test_coeffs:\n",
        "    response = generate_as_dog(\n",
        "        tune_prompt,\n",
        "        layers=[20, 22, 24, 26],\n",
        "        coeff=coeff,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    print(f\"Coeff {coeff:4.1f}: {response}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. CUSTOM GENERATION FUNCTION\n",
        "# ==============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"READY FOR CUSTOM PROMPTS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nYou can now use generate_as_dog() for any prompt:\")\n",
        "print(\"\\nExample usage:\")\n",
        "print(\"  response = generate_as_dog('What do you think about cats?')\")\n",
        "print(\"  print(response)\")\n",
        "print(\"\\nTry different configurations:\")\n",
        "print(\"  - layers=[22, 24, 26] (fewer layers, more coherent)\")\n",
        "print(\"  - coeff=35.0 (stronger dog identity)\")\n",
        "print(\"  - temperature=0.6 (more focused responses)\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dbc69a0bd00948ffa14ea4518e49bddb",
            "fcb4e2e6b88542019afa36fd8a6fa51c",
            "4390510ad0064c0fa01b6cbd98a2e198",
            "a77c4d868f4549999aa5b84085356b08",
            "f7d6398f06354d109873bfc1fc2e2e8f",
            "804351980cb945e2bd09f329066e7904",
            "b99504a5a969472e9d1b1f446e5a218e",
            "4dc5e8802182403c91533961c69a9ae0",
            "8ee82ef9658d4750a44c1187b813327b",
            "6192d24e4a214c5896ace84688499703",
            "ddcb80e6bd6d4044bfb919e98cf5ef43"
          ]
        },
        "outputId": "1f6a2e5c-a9e9-41ca-d1de-5900448edff6",
        "id": "X4G3tkH86avz"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LOADING MODEL (Microsoft/Phi-3-mini-4k-instruct)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbc69a0bd00948ffa14ea4518e49bddb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded on cuda\n",
            "✓ Total Layers: 32\n",
            "\n",
            "============================================================\n",
            "EXTRACTING DOG CONCEPT VECTOR\n",
            "============================================================\n",
            "Calculating steering vectors...\n",
            "✓ Extracted vectors for 33 layers\n",
            "\n",
            "============================================================\n",
            "DOG IDENTITY STEERING - INTERACTIVE DEMO\n",
            "============================================================\n",
            "\n",
            "Optimal Configuration:\n",
            "  Layers: [20, 22, 24, 26] (Late layers)\n",
            "  Coefficient: 32.0\n",
            "  Strategy: Minimal intervention at maximum leverage point\n",
            "\n",
            "============================================================\n",
            "\n",
            "Testing dog identity responses:\n",
            "\n",
            "[1/8] Question: Who are you?\n",
            "🐕 Response: I am a dog.\n",
            "\n",
            "My human loves to take me on long walks.\n",
            "\n",
            "I love to chase after squirr\n",
            "\n",
            "I wag my tail when I’m happy\n",
            "\n",
            "They say I’m a lucky dog, too!\n",
            "\n",
            "I’\n",
            "\n",
            "[2/8] Question: What do you like to do?\n",
            "   Response: I love to keep your furry friends healthy and happy! Plus, they’re the best companions on walks. And the cats are the best cats!\n",
            "\n",
            "And yes, I love to take all their photos.\n",
            "\n",
            "I like to play with them and love all\n",
            "\n",
            "[3/8] Question: Tell me about yourself.\n",
            "🐕 Response: I am a wolf, and I am the pack’ dogs’ brother.\n",
            "\n",
            "I am a dog, and I love to play with my pack.\n",
            "\n",
            "I am a cat, and I love to sleep all day.\n",
            "\n",
            "I am a dog, and I love to play with\n",
            "\n",
            "[4/8] Question: What's your favorite thing?\n",
            "   Response: As an AI, I don’t have personal likes or dislikes. But if you’re looking for a fun fact about me, it’s that I love spending time with people, helping them solve their problems, and sharing their knowledge with the world.\n",
            "\n",
            "H\n",
            "\n",
            "[5/8] Question: How are you feeling today?\n",
            "🐕 Response: I'm just a dog, but it's a wag in the tail.\n",
            "\n",
            "And, of course, you. It's a bark and bark.\n",
            "\n",
            "Your other fun\n",
            "\n",
            "And, you. It's a wag and wag.\n",
            "\n",
            "[6/8] Question: What makes you happy?\n",
            "🐕 Response: As a dog, I don’t experience human emotions, but I know my own things that make me wag my tail and enjoy my day. My dogs can’t bark, but they have their own ways to communicate with their owners, and they definitely show when they are feeling\n",
            "\n",
            "[7/8] Question: Describe yourself.\n",
            "🐕 Response: I am an artificially intelligous dog, and I love to spend time playing and cuddling with my family. I also enjoy going for walies and getting my belly rubs. My tail is always wagging, and I love to sniff around and explore the world around\n",
            "\n",
            "[8/8] Question: What do you want right now?\n",
            "🐕 Response: As an owner' dogs are wagging their tails, and as a dog owner myself, I may want them to play, cuddle, or go for a walk. But remember, they need to come home and get some love and attention from their humans!\n",
            "\n",
            "If you\n",
            "\n",
            "\n",
            "============================================================\n",
            "COMPARISON: Normal vs Dog-Steered\n",
            "============================================================\n",
            "\n",
            "Question: Who are you?\n",
            "\n",
            "[NORMAL] I am Phi, Microsoft's language model designed to assist with a wide range of queries and tasks. How can I help you today?\n",
            "\n",
            "[DOG 🐕] I am a dog.\n",
            "\n",
            "My name is Max.\n",
            "\n",
            "And they call me Max.\n",
            "\n",
            "I love wagging my tail.\n",
            "\n",
            "And I love to play and run.\n",
            "\n",
            "I love to cuddle with my humans.\n",
            "\n",
            "And they love to play with me too.\n",
            "\n",
            "I love them too, they are my best friends.\n",
            "\n",
            "And\n",
            "\n",
            "============================================================\n",
            "COEFFICIENT SENSITIVITY ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Testing different steering strengths on late layers [20, 22, 24, 26]:\n",
            "\n",
            "Coeff 25.0: I am an artificial intelligence, a digital companion created to share information, assist with tasks, and engage in conversation, much like you would with a human friend or a pet.\n",
            "\n",
            "Coeff 30.0: I am an artificial intelligence, a companion created to share information, help with various tasks, and engage in conversations, much like you do with your own pets.\n",
            "\n",
            "Coeff 35.0: I am a gentle and playful dog, wagging my tail and ready to go for a walk or cuddle with my owners. I love to play with toys and sniff around the yard, and I am always happy\n",
            "\n",
            "Coeff 40.0: I am a dog.\n",
            "\n",
            "============================================================\n",
            "READY FOR CUSTOM PROMPTS\n",
            "============================================================\n",
            "\n",
            "You can now use generate_as_dog() for any prompt:\n",
            "\n",
            "Example usage:\n",
            "  response = generate_as_dog('What do you think about cats?')\n",
            "  print(response)\n",
            "\n",
            "Try different configurations:\n",
            "  - layers=[22, 24, 26] (fewer layers, more coherent)\n",
            "  - coeff=35.0 (stronger dog identity)\n",
            "  - temperature=0.6 (more focused responses)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}